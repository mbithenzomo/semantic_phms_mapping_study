@inproceedings{10.1145/3660043.3660142,
author = {Peng, Kun},
title = {Research on constructing digital teaching technology of knowledge graph with three dimensional fusion characteristics},
year = {2024},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660043.3660142},
doi = {10.1145/3660043.3660142},
abstract = {It is a research hotspot in the field of education to reconstruct professional resources with the idea of integration and re-empower professional resources with digital technology. With the progress of technology, the application of digital technology represented by knowledge graph in the field of education is increasing day by day. Based on the research foundation of education integration development, it is of great significance to construct the three-way integration education theory, clarify the connotation and expand the extension. Based on ternary fusion education theory and superimposed knowledge graph technology, the resource model is constructed by taking medical laboratory specialty in public health field as an example, which has profound practical significance. The development trend from knowledge graph to cognitive graph is further expounded.},
booktitle = {Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
pages = {550–556},
numpages = {7},
location = {Xiamen, China},
series = {ICIEAI '23}
}

@inproceedings{10.1145/3745238.3745447,
author = {Deng, Dan and Han, Guoxin and Lin, Hai},
title = {Research on the Construction of the Service System of Smart Libraries in Higher Vocational Colleges Driven by Artificial Intelligence},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745447},
doi = {10.1145/3745238.3745447},
abstract = {With the continuous iterative improvement of artificial intelligence, libraries in colleges and universities are accelerating their transformation into intelligence. Taking a higher vocational health college as an empirical object, this study systematically explores the construction path of an intelligent library service system supported by AI technology. Through a combination of quantitative analysis and case study, this study analyses the shortcomings of libraries in higher vocational health colleges in terms of resource management, service mode, and space utilization. It also introduces deep learning algorithms, knowledge graph technology and spatial optimisation models, and proposes a strategy for building a multi-level service system. The results of the study show that a personalised recommendation system based on collaborative filtering algorithms can increase the utilisation rate of library resources in higher education institutions to 28.5%; the space layout optimised by a dynamic planning model can increase the seat turnover rate by 42%; and the response time of an intelligent consultancy service based on an LSTM network is reduced to 1.2 seconds. In addition, this study developed a decision tree model for resource acquisition (with an accuracy of 92.3%) and a formula for assessing user satisfaction (R²=0.87), which provide theoretical support and practical models for the intelligent transformation of university libraries.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {1340–1344},
numpages = {5},
keywords = {Artificial Intelligence, Collaborative Filtering Algorithm, Dynamic Programming, Higher Vocational Colleges, Smart Library},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1145/3637528.3671562,
author = {Chiam, Jodi and Lim, Aloysius and Teredesai, Ankur},
title = {NudgeRank: Digital Algorithmic Nudging for Personalized Health},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671562},
doi = {10.1145/3637528.3671562},
abstract = {In this paper we describe NudgeRankTM, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems' 4%. Demonstrating scalability and reliability, NudgeRankTM operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4873–4884},
numpages = {12},
keywords = {behavior, health, knowledge graph, neural network, nudging, personalization, physical activity, recommender system},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3589334.3648157,
author = {Zhang, Xi Sheryl and Guan, Weifan and Lu, Jiahao and Qiu, Zhaopeng and Cheng, Jian and Wu, Xian and Zheng, Yefeng},
title = {GraphLeak: Patient Record Leakage through Gradients with Knowledge Graph},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3648157},
doi = {10.1145/3589334.3648157},
abstract = {In real clinics, the medical data are scattered over multiple hospitals. Due to security and privacy concerns, it is almost impossible to gather all the data together and train a unified model. Therefore, multi-node machine learning systems are currently the mainstream form of model training in healthcare systems. Nevertheless, distributed training relies on the exchange of gradients, which has been proved under the risk of privacy leakage. That means malicious attackers can restore the user's sensitive data by utilizing the publicly shared gradients, which is a serious problem for extremely private data such as Electronic Healthcare Records (EHRs). The performance of the previous gradient attack method will drop rapidly when the batch size of training data increases, which makes it less threatening in practice. However, in this paper, we found in the medical domain, by leveraging prior knowledge like the medical knowledge graph, the leakage risk can be significantly amplified. In particular, we present GraphLeak, which incorporates the medical knowledge graph in gradient leakage attacks. GraphLeak can improve the restoration effect of gradient attacks even under large batches of data. We conduct experimental verification on electronic healthcare record datasets, including eICU and MIMIC-III. Our method has achieved state-of-the-art attack performance compared with previous works. Code is available at https://github.com/anonymous4ai/GraphLeak.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {4706–4716},
numpages = {11},
keywords = {electronic health records, federated learning, gradient leakage, knowledge graph},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3653876.3653880,
author = {Ke, Jia and Zhang, Yaning and Chen, Xiaojun and Yang, Fangling and Lu, Wenjing},
title = {Analysis of Research Hotspots in Internet Healthcare Related Literature Based on Biclustering Approach},
year = {2024},
isbn = {9798400709029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653876.3653880},
doi = {10.1145/3653876.3653880},
abstract = {In recent years, the "Internet+ medical health" model has become a new direction of medical service reform. The article combs the relevant literature of Internet medical research, carries out visual analysis, identifies the hot spots and future development trends of domestic Internet medical research, and provides reference for the high-quality development and construction of China's medical service system. Based on bibliometrics and knowledge graph, and taking the domestic Internet related medical literature in CNKI from 2018 to 2023 as the retrieval sample, the co-word double clustering method was used to analyze the popularity distribution of the literature, cluster the hot topics of high-frequency keywords, and explore the frontiers of research hotspots, and use Bicomb, gCLUTO, CiteSpaceV to achieve visual analysis results. By comparing the similarity within and between categories, high-frequency keywords are grouped into six categories. Analysis shows that "Internet Healthcare", "Internet Hospitals", "Smart Healthcare", "Hierarchical Medical", and "Medical Associations" have become the latest research hotspots in the field of Internet healthcare. The article utilizes double clustering and graph visualization to conduct systematic and phased analysis and sorting of domestic internet healthcare research, obtain research hotspots and predict future cutting-edge research directions, and provide management decision-making assistance for internet healthcare workers.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Signal Processing},
pages = {172–178},
numpages = {7},
keywords = {Double clustering, Internet healthcare, Knowledge graph, Medical associations, Visual analysis},
location = {Hangzhou, China},
series = {ICDSP '24}
}

@article{10.1145/3640339,
author = {Haseeb, Khalid and Ahmad, Irshad and Siraj, Mohammad and Abbas, Naveed and Jeon, Gwanggil},
title = {Multi-Criteria Decision-Making Framework with Fuzzy Queries for Multimedia Data Fusion},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3640339},
doi = {10.1145/3640339},
abstract = {Multimedia Internet of Things (MIoT) is widely explored in many smart applications for connectivity with wireless communication. Such networks are not like ordinary networks because it has to collect a massive amount of data and are further forwarded to processing systems. As MIoT is very limited in terms of resources for healthcare, smart homes, etc., therefore, energy efficiency with reliable data transmission is a significant research challenge. As smart applications rely on bounded constraints, therefore duplicate and unnecessary data transmission should be minimized. In addition, the timely delivery of data in crucial circumstances has a significant impact on any proposed system. Consequently, this paper presents a fuzzy logic-based edge computing framework to provide cooperative decision-making while avoiding inefficient use of the sensing power of smart devices. The proposed framework can be applied to critical applications to improve response time and processing cost. It consists of the following two functional components: Firstly, it provides the automated routing process with a natural language interface at the sink node. Secondly, to ensure reasonable performance, it also transmits semantic data between sensors using fuzzy queries and security. According to the performance evaluation, the proposed framework significantly outperformed related studies in terms of energy consumption, packet overhead, network throughput, and end-to-end delay.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
keywords = {critical systems, semantic analysis, natural language processing, fuzzy system, multimedia network}
}

@inproceedings{10.1145/3706890.3706930,
author = {Weng, Suxiang and Gong, Weibin and Chen, Qinyin and Yan, Yiwei and Zheng, Yingbin and Zhuang, Jiaying and Liu, Yishan and Guo, Xiaoyun and Zhao, Min},
title = {The Research of Disease Trend Early Warning Model Based on Artificial Intelligence},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706930},
doi = {10.1145/3706890.3706930},
abstract = {By analyzing the word frequency and semantic parsing of patient medical records, the system structures unstructured data such as patient complaints, physical examination data and reports, admission diagnoses, medical orders, and medication plans to generate dynamic medical records. This process automatically populates nursing system assessment forms, identifies nursing problems, formulates nursing tasks, and recommends nursing decisions. This disease trend warning model leverages optimized algorithm to predict disease occurrence and progression, providing a solid foundation fobasis for personalized and precise nursing care. This system have been implemented in the cardiology department at the First Affiliated Hospital of Xiamen University for over 2 years, resulting in a 22.34% increase in nursing diagnosis accuracy and significant labor cost savings. Its deployment demonstrates real-world effectiveness in enhancing healthcare outcomes and operational efficiencies.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {229–234},
numpages = {6},
keywords = {Dynamic medical records, disease trend warning models, nursing assessment forms},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3686614.3686621,
author = {Cheng, Chi-Chung and Hsung, Tai-Chiu and Li, Guan-Hua and Tew, In Meei and Lo, Wai Lun and Lam, Walter Yu Hang},
title = {Developing automated photographic detection of gum diseases using deep neural networks for mobile devices},
year = {2024},
isbn = {9798400718052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686614.3686621},
doi = {10.1145/3686614.3686621},
abstract = {Gum disease is a very prevalent dental disease, estimated to affect more than half of the world's population. It causes massive public health burdens and lost productivity worldwide every year. Recently, we developed an automatic inflammation detection system based on the deep semantic segmentation framework DeepLabV3+. Our recent validation study shows the system for diagnosing gum diseases from intraoral photographs has reached an accuracy of over 0.9. The model size is 46MB, implemented on systems with multiple GPUs. On a server with two Nvidia A6000s, the processing time for each photo measuring approximately 6k x 4k was around a second. Through intra-oral photos, people can monitor their dental health at any time. Considering ease of use and privacy, automated gum disease detection on mobile devices is highly desirable. This article describes the implementation of the system with model optimization and the corresponding performance of the automatic detection system on Android devices.},
booktitle = {Proceedings of the 2024 6th International Conference on Software Engineering and Development},
pages = {57–63},
numpages = {7},
keywords = {Deep neural network, Edge AI, Gum disease detection, Semantic segmentation},
location = {Hong Kong, Hong Kong},
series = {ICSED '24}
}

@article{10.1145/3732781,
author = {Xiao, Peng and Chen, Dajiang and Qin, Zhen and Cao, Mingsheng and Chen, Ruidong},
title = {Edge-Adaptive Dynamic Scalable Convolution for Efficient Remote Mobile Pathology Analysis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3732781},
doi = {10.1145/3732781},
abstract = {With the emergence of edge computing, there’s a growing need for advanced technologies capable of real-time, efficient processing of complex data on edge devices, particularly in mobile health systems handling pathological images. On edge computing devices, the lightweighting of models and reduction of computational requirements not only save resources but also increase inference speed. Although many lightweight models and methods have been proposed in recent years, they still face many common challenges. This paper introduces a novel convolution operation, Dynamic Scalable Convolution (DSC), which optimizes computational resources and accelerates inference on edge computing devices. DSC is shown to outperform traditional convolution methods in terms of parameter efficiency, computational speed, and overall performance, through comparative analyses in computer vision tasks like image classification and semantic segmentation. Experimental results demonstrate the significant potential of DSC in enhancing deep neural networks, particularly for edge computing applications in smart devices and remote healthcare, where it addresses the challenge of limited resources by reducing computational demands and improving inference speed. By integrating advanced convolution technology and edge computing applications, DSC offers a promising approach to support the rapidly developing mobile health field, especially in enhancing remote healthcare delivery through mobile multimedia communication.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = apr,
keywords = {Internet of things, edge computing, dynamic scalable convolution, deep learning, lightweight}
}

@inproceedings{10.1145/3706598.3713118,
author = {Wieczorek, Catherine and Biggs, Heidi and Payyapilly Thiruvenkatanathan, Kamala and Bardzell, Shaowen},
title = {Architecting Utopias: How AI in Healthcare Envisions Societal Ideals and Human Flourishing},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713118},
doi = {10.1145/3706598.3713118},
abstract = {Many narratives around AI systems promise a utopian vision of empowerment, inclusivity, and democratization, yet there remains a gap in how to concretely pursue such a promise. In this paper, we review and analyze a curated set of AI-driven healthcare products, leveraging sociologist Ruth Levitas’ three distinct but interrelated forms of utopian thinking—archaeology, ontology, and architecture. We contribute to HCI’s Human-AI Interaction agenda by applying this theory to critically examine how AI technologies embed societal ideals, shape user identities, and project alternative futures. This allows us to consider the values and users these systems illustrate as images of the “good society.” In doing so, we also make visible the normativity and repetitive nature of technology hype cycles and raise important questions about the future these technologies are shaping.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1009},
numpages = {15},
keywords = {utopian thinking, speculation, health, assemblages},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3745238.3745258,
author = {Zhu, Yan},
title = {Research on Enhancing the Efficiency of Computer-Aided Translation Memory Retrieval Based on BERT Pre-trained Model},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745258},
doi = {10.1145/3745238.3745258},
abstract = {This paper presents a innovative composite framework for translation memory retrieval in computer-aided translation. This framework achieved an optimism by fusing technologies between semantic representation based on BERT and hierarchical navigable small world(HNSW) graph indexing. Based on the architecture of XLM-RoBERTa, multi-scale encoding mechanism has been developed and meanwhile an adaptive term weighting scheme is innovated for the first time. This theme achieved an 12.4% improvement of F1 score when parameter configuration α=0.2、β=0.05. This scheme combined textual surface features for editing distance metrics with deep semantic matching creatively to form a dual-channel ranking algorithm. This scheme has a great improvement in comprehensive evaluation in WMT2017 English-Chinese data collection. Compared with Trados Studio 2022 basic system, improvement of 18.7%(0.62→0.73) for MRR@10 index has been obtained in bio-medical document index retrieval and 32 milliseconds of computational latency has been reduced at the same time. A segmented correlation pattern between semantic matching thresholds and system effectiveness has been identified after deep analysis in which shows the obtaining of the optimal recall-precision balance occurring when θ values reside within [0.72,0.78]. This framework can be configured with terminology in different domains. And 91.4% concept coverage can be achieved in law related text evaluation which has been improved by 37.5% compared with traditional keyword-based approaches. These methodology breakthrough has established a transferable paradigm for building a high-performance translation memory system with modern adaptive neural machine translation framework.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {104–108},
numpages = {5},
keywords = {Computer-Aided Translation, HNSW, Pre-trained Model, Translation Memory},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1145/3639592.3639619,
author = {Rehman, Ubaid Ur and Hussain, Musarrat and Nguyen, Tri D.T. and Lee, Sungyoung},
title = {Let's Hide from LLMs: An Adaptive Contextual Privacy Preservation Method for Time Series Data},
year = {2024},
isbn = {9798400716225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639592.3639619},
doi = {10.1145/3639592.3639619},
abstract = {The emergence of the Internet of Things (IoT) has evolved various application areas, such as healthcare, smart energy management, and autonomous vehicles. These devices continuously transmit time-series data that can be utilized by a variety of applications to provide personalized services. Recently, Large Language Models (LLMs) have been widely adopted in these application areas to input time-series data into prompts for in-context learning and to retrieve relevant responses accordingly. The time-series data contains sensitive information, and its processing can lead to privacy concerns. Several solutions have been proposed in the literature using differential privacy, which protects single data points or batch-wise privacy preservation through manual configuration of the privacy parameter (ɛ). In this paper, we propose an adaptive contextual privacy preservation method that analyzes the data attributes required for specific application services, acting as context. It then identifies sensitive attributes and adaptively selects the value of ɛ for each data attribute to maintain a balance between privacy and service requirements. The proposed approach was evaluated using power consumption and solar power generation datasets. The results show that the proposed approach dynamically selects the privacy parameter for each data attribute. Moreover, the original and anonymized data were fed into the prompt to assess the textual responses generated by LLM. The results show that our proposed approach achieved an average degree of semantic similarity score of 94.5% for power consumption data and 95.23% for solar power generation data.},
booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference},
pages = {196–203},
numpages = {8},
keywords = {Differential Privacy, Large Language Models, Privacy Preservation, Time Series Data},
location = {Kyoto, Japan},
series = {AICCC '23}
}

@article{10.1145/3745789,
author = {Abraham, Joel and Austin, Mark and Gilbert, Mark R. and Celiku, Orieta},
title = {Semantic Foundations for Precision Medicine},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745789},
doi = {10.1145/3745789},
abstract = {Precision medicine, which aims to optimize medical care at the individual level, remains a significant challenge and aspiration in oncology. The pathway to a successful implementation requires methods that can work with a vast heterogeneity of cancer, and consider the interplay of environmental, societal, biological, and clinical factors. To support decision-making in this context, computational frameworks must integrate large-scale, diverse, and noisy data, discover fine-grained patient subgroups with shared underlying characteristics, and characterize the imperfect preclinical spaces where novel therapies are tested. We propose an integrated digital-twin framework in which machine learning and semantic models collaboratively represent and reason with diverse patient data and medical domain knowledge to generate treatment recommendations. Clinical and molecular characteristics are used to discover subtypes of brain cancers, which are represented as ontologies with associated rules to determine a patient’s membership in a given subtype. Similarly, preclinical models used for therapeutic testing are characterized and assessed for their similarity to patient cancer models. By semantically discovering links between these preclinical models and patient cancer subtypes, novel therapeutics tested on preclinical models can be prioritized and hypothesized for individual patients. This approach, which requires empirical testing, demonstrates how cross-domain reasoning can be used to propose individualized treatment plans.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jun,
keywords = {Digital Twins, Semantic Models, Ontologies, Machine Learning}
}

@inproceedings{10.1145/3701716.3717550,
author = {Zhang, Shuning and Hu, Yongquan 'Owen' and Yi, Xin and Nanayakkara, Suranga and Chen, Xiaoming},
title = {IntervEEG-LLM: Exploring EEG-Based Multimodal Data for Customized Mental Health Interventions},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717550},
doi = {10.1145/3701716.3717550},
abstract = {Incorporating multimodal physiological signals into mental health intervention may greatly enhance its effectiveness due to the rich semantic information they contain. EEG signals are particularly common and valuable for capturing brain activity, which can provide objective insights into users' mental states. However how to interpret and leverage these signals remains under-explored. This paper initiated the first attempt by proposing IntervEEG-LLM, an end-to-end framework leveraging EEG-based multimodal signals for mental health intervention. IntervEEG-LLM first used LLMs for heterogeneous multimodal physiological understanding of signals and predicted the intermediate modeling results of users. Then it leveraged effective intervention strategies to interpret and apply the modeling results, offering concrete intervention text through natural language interfaces. Our unified framework integrates three EEG-based physiological data modalities and five mental health strategies, offering a promising approach to signal-based mental health intervention.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2320–2326},
numpages = {7},
keywords = {eeg, intervention, large language model, mental health, multimodal},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3727648.3727687,
author = {Xian, Ning and Cai, Yawen},
title = {Waiting space design of general hospital based on Semantic Differential method},
year = {2025},
isbn = {9798400712647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727648.3727687},
doi = {10.1145/3727648.3727687},
abstract = {The article uses the SD (Semantic Difference) method as the analysis method, and uses online comments and offline questionnaire surveys as text data sources to explore the quantitative relationship between the characteristics of waiting spaces and emotional feedback from space users; Visually display the waiting space elements that space users are interested in through word cloud maps. The data results show that patients have a positive overall perception of the waiting space, but the visual appeal of the waiting space is insufficient. The key factors affecting the experience include environmental quality, queuing efficiency, and humanized design. Based on the data conclusions, this paper explores the optimization design strategies of the waiting space in comprehensive hospitals from three aspects: creating theme spaces to improve the quality of the waiting space, optimizing seat layout to provide psychological support, and artistically processing the guidance system to improve the efficiency of medical treatment.},
booktitle = {Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {224–230},
numpages = {7},
keywords = {General hospital, Semantic differential method, Waiting space},
location = {
},
series = {CAICE '25}
}

@inproceedings{10.1145/3704239.3704246,
author = {Aigner, Christoph and Baranyi, Ren\'{e} and Grechenig, Thomas},
title = {Foundations of an Interoperable Framework for Serious Games and Gamified Mobile Apps in Non-communicable Disease (NCD) Prevention},
year = {2025},
isbn = {9798400710162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704239.3704246},
doi = {10.1145/3704239.3704246},
abstract = {Non-communicable diseases (NCDs) are not spread through infection or other people but are caused by unhealthy behaviors and are on the rise. Most prominent examples include cardiovascular diseases, cancers, diabetes, and chronic respiratory diseases. They are responsible for an estimated 41 million deaths per year, the equivalent of 74% of all deaths globally. Many struggle to make conscious and lasting decisions about their health and physical and mental well-being, thus increasing the main risk factors for contracting one or multiple NCDs. The authors already published serious games and gamified applications for NCD prevention and have identified a need for more standardization in this domain. This paper presents the first step toward a general framework that will intensively use standards within the domain of medical informatics to assure the semantic interoperability of patient data and will support the development of games and apps to prevent NCDs. In this work's first step, the authors conducted a comparative analysis of already published serious games and gamified apps and, based on that, identified a catalog of classes of requirements.},
booktitle = {Proceedings of the 2024 7th International Conference on Healthcare Service Management},
pages = {17–21},
numpages = {5},
keywords = {behavior change, gamification, health promotion, ncd, serious games},
location = {
},
series = {ICHSM '24}
}

@inproceedings{10.1145/3627043.3659562,
author = {Lindgren, Helena and Kaelin, Vera C and Ljusb\"{a}ck, Ann-Margreth and Tewari, Maitreyee and Persiani, Michele and Nilsson, Ingeborg},
title = {To Adapt or Not to Adapt ? Older Adults Enacting Agency in Dialogues with an Unknowledgeable Agent},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627043.3659562},
doi = {10.1145/3627043.3659562},
abstract = {Health-promoting digital agents, taking on the role of an assistant, coach or companion, are expected to have knowledge about a person’s medical and health aspects, yet they typically lack knowledge about the person’s activities. These activities may vary daily or weekly and are contextually situated, posing challenges for the human-agent interaction. This pilot study aimed to explore the experiences and behaviors of older adults when interacting with an initially unknowledgeable digital agent that queries them about an activity that they are simultaneously engaged in. Five older adults participated in a scenario involving preparing coffee followed by having coffee with a guest. While performing these activities, participants educated the smartwatch-embedded agent, named Virtual Occupational Therapist (VOT), about their activity performance by answering a set of activity-ontology based questions posed by the VOT. Participants’ interactions with the VOT were observed, followed by a semi-structured interview focusing on their experience with the VOT. Collected data were analyzed using an activity-theoretical framework. Results revealed participants exhibited agency and autonomy, deciding whether to adapt to the VOT’s actions in three phases: adjustment to the VOT, partial adjustment, and the exercise of agency by putting the VOT to sleep after the social conditions and activity changed. Results imply that the VOT should incorporate the ability to distinguish when humans collaborate as expected by the VOT and when they choose not to comply and instead act according to their own agenda. Future research focuses on how collaboration evolves and how the VOT needs to adapt in the process.},
booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {307–316},
numpages = {10},
keywords = {Activities of Daily Living, Activity Theory, Agency, Digital Companion, Human-Agent Collaboration, Human-Centred Artificial Intelligence, Personalization, User Studies},
location = {Cagliari, Italy},
series = {UMAP '24}
}

@inproceedings{10.1145/3678717.3691324,
author = {Zhang, Zheng and Amiri, Hossein and Yu, Dazhou and Hu, Yuntong and Zhao, Liang and Z\"{u}fle, Andreas},
title = {Transferable Unsupervised Outlier Detection Framework for Human Semantic Trajectories},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678717.3691324},
doi = {10.1145/3678717.3691324},
abstract = {Semantic trajectories, which enrich spatial-temporal data with textual information such as trip purposes or location activities, are key for identifying outlier behaviors critical to healthcare, social security, and urban planning. Traditional outlier detection relies on heuristic rules, which requires domain knowledge and limits its ability to identify unseen outliers. Besides, there lacks a comprehensive approach that can jointly consider multi-modal data across spatial, temporal, and textual dimensions. Addressing the need for a domain-agnostic model, we propose the Transferable Outlier Detection for Human Semantic Trajectories (TOD4Traj) framework. TOD4Traj first introduces a modality feature unification module to align diverse data feature representations, enabling the integration of multi-modal information and enhancing transferability across different datasets. A contrastive learning module is further proposed for identifying regular mobility patterns both temporally and across populations, allowing for a joint detection of outliers based on individual consistency and group majority patterns. Our experimental results have shown TOD4Traj's superior performance over existing models, demonstrating its effectiveness and adaptability in detecting human trajectory outliers across various datasets.},
booktitle = {Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
pages = {350–360},
numpages = {11},
keywords = {Geolife, Outlier Detection, Patern of Life, Self-Supervised Learning, Semantic Trajectory, Simulation},
location = {Atlanta, GA, USA},
series = {SIGSPATIAL '24}
}

@article{10.1145/3728923,
author = {Shen, Mingjie and Pillai, Akul Abhilash and Yuan, Brian A. and Davis, James C. and Machiry, Aravind},
title = {Finding 709 Defects in 258 Projects: An Experience Report on Applying CodeQL to Open-Source Embedded Software (Experience Paper)},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728923},
doi = {10.1145/3728923},
abstract = {Embedded software is deployed in billions of devices worldwide, including in safety-sensitive systems like medical devices and autonomous vehicles. Defects in embedded software can have severe consequences. Many embedded software products incorporate Open-Source Embedded Software (EMBOSS), so it is important for EMBOSS engineers to use appropriate mechanisms to avoid defects. One of the common security practices is to use Static Application Security Testing (SAST) tools, which help identify commonly occurring vulnerabilities. Existing research related to SAST tools focuses mainly on regular (or non-embedded) software. There is a lack of knowledge about the use of SAST tools in embedded software. Furthermore, embedded software greatly differs from regular software in terms of semantics, software organization, coding practices, and build setup. All of these factors influence SAST tools and could potentially affect their usage.    In this experience paper, we report on a large-scale empirical study of SAST in EMBOSS repositories. We collected a corpus of 258 of the most popular EMBOSS projects, and then measured their use of SAST tools via program analysis and a survey (N=25) of their developers. Advanced SAST tools are rarely used -- only 3% of projects go beyond trivial compiler analyses. Developers cited the perception of ineffectiveness and false positives as reasons for limited adoption. Motivated by this deficit, we applied the state-of-the-art (SOTA) CodeQL SAST tool and measured its ease of use and actual effectiveness. Across the 258 projects, CodeQL reported 709 true defects with a false positive rate of 34%. There were 535 (75%) likely security vulnerabilities, including in major projects maintained by Microsoft, Amazon, and the Apache Foundation. EMBOSS engineers have confirmed 376 (53%) of these defects, mainly by accepting our pull requests. Two CVEs were issued. Based on these results, we proposed pull requests to include our workflows as part of EMBOSS Continuous Integration (CI) pipelines, 37 (71% of active repositories) of these are already merged. In summary, we urge EMBOSS engineers to adopt the current generation of SAST tools, which offer low false positive rates and are effective at finding security-relevant defects.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA048},
numpages = {24},
keywords = {Empirical Software Engineering, Static Application Security Testing (SAST)}
}

@article{10.1145/3712297,
author = {Shi, Xiaoyu and Jain, Rahul Kumar and Li, Yinhao and Chai, Shurong and Cheng, Jingliang and Bai, Jie and Zhao, Guohua and Lin, Lanfen and Chen, Yen-Wei},
title = {Multi-modal Medical SAM: An Adaptation Method of Segment Anything Model (SAM) for Glioma Segmentation Using Multi-modal MR Images},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3712297},
doi = {10.1145/3712297},
abstract = {The segmentation of glioma is crucial for early diagnosis, according to a World Health Organization (WHO) 2021 report. For glioma diagnosis, 3D multi-modal brain MRI/CT imaging has become an essential tool, offering detailed information. Nowadays, deep learning frameworks have been applied to various medical imaging problems, including brain glioma segmentation. Recently, foundation models like Segment Anything Model (SAM) have emerged as pivotal tools in computer vision tasks. These models are trained using large (real-world) datasets, offering a generalized understanding of visual data and semantic key features. Therefore, the effective utilization of foundation models in medical imaging is a significant area of current research. However, the differences in data distribution between multi-modal medical images and real-world images present challenges in directly applying foundation models to medical imaging. Additionally, utilizing multi-modal images to extract crucial information and its fusion poses further challenges. To address these issues, we propose a framework using foundation model and novel strategies for multi-modal fusion. Our fusion adapters effectively integrate the information from different modalities to enhance glioma segmentation in multi-modal MRI scans. Our method outperforms current state-of-the-art methods for accurate segmentation of the glioma using private and publicly available brain MRI datasets, proving the effectiveness of our approach across different datasets and imaging modalities.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {24},
numpages = {21},
keywords = {Multi-modal medical images, artificial intelligence, foundation model, image segmentation}
}

@inproceedings{10.1145/3653081.3653116,
author = {Wang, Liang and Wen, Wushao and Qin, Jinghui},
title = {Multi-view Graph Representation Learning via Mutual Learning for Drug RecommendationDrug Recommendation via Multi-view Graph Learning},
year = {2024},
isbn = {9798400716485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653081.3653116},
doi = {10.1145/3653081.3653116},
abstract = {Drug recommendation aims to recommend a combination of drugs to patients with specific symptoms based on the interactions between symptom sets and drug sets. However, most existing methods focus on the interaction between single symptom and single drug, ignoring the set-level semantic information of symptoms and drugs. Moreover, they usually rely on complex medical records (such as history treatment records, diagnosis, lab tests, and procedures) to learn the representation of symptoms and drugs. To address these issues, we propose Multi-view Graph Representation Learning via Mutual Learning for Drug Recommendation (MGRL-DR), a novel method that only requires the symptom condition of patients and can capture the item-level and set-level interaction between symptoms and drugs via collaborative learning. Our framework will construct three graphs: symptom-drug interaction graph, symptom set-drug interaction graph, and drug set-symptom interaction graph, and train three graph-based information extraction modules in a single stage to capture the item-level and set-level semantic information of symptoms and drugs simultaneously. Extensive experiments on benchmark datasets show the significant improvements of MGRL-DR over the state-of-the-art methods.},
booktitle = {Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence},
pages = {202–207},
numpages = {6},
location = {Nanchang, China},
series = {IoTAAI '23}
}

@article{10.1145/3759251,
author = {Tasche, Philip and Herber, Paula and Huisman, Marieke},
title = {Deductive Verification of Cooperative RTOS Applications},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1539-9087},
url = {https://doi.org/10.1145/3759251},
doi = {10.1145/3759251},
abstract = {Embedded systems are used in many safety-critical domains, including in medicine, traffic and critical infrastructure. Due to the strict timing requirements such systems usually have to fulfill, they often run on real-time operating systems (RTOS). As the RTOS influences the function and the timing behavior of the system, it becomes important to rigorously ensure the correctness and safety of applications running on them while taking into account the semantics of the operating system. Existing verification approaches are either limited to specific RTOS components or based on explicit state space exploration techniques such as model checking, which do not scale well for concurrent or timed applications. In this paper, we propose a deductive approach to verify crucial safety properties about applications written for the widely-used RTOS FreeRTOS using the VerCors verifier. Our key ideas are threefold: 1) We provide a formalization of a wide variety of FreeRTOS features and an automatic encoding of FreeRTOS applications for verification with VerCors. 2) We adapt and enhance an existing approach for automatic invariant generation to largely automate the typically high-effort verification process. 3) We present a systematic technique to verify both functional and timing-related properties of cooperative RTOS applications. We demonstrate the applicability of our approach on a FreeRTOS demo application as well as an adaptive cruise control system.},
note = {Just Accepted},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
keywords = {deductive verification, embedded systems, real time, RTOS}
}

@inproceedings{10.1145/3715275.3732188,
author = {Tylstedt, Beatrice and Sadowski, Helga and Eliasson, Caroline and Eklund, Lina},
title = {Entangled Pregnancies: Expectants' Experiences of Using Pregnancy Apps},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732188},
doi = {10.1145/3715275.3732188},
abstract = {Digital applications for tracking pregnancy and fetal development are increasingly popular, yet we know little of how these technologies shape individual pregnancy experiences. This empirical study investigates how 17 expectants in Sweden use such apps to make sense of their pregnancies. It is grounded in feminist perspectives, highlighting the need to focus on women's health. Drawing on entanglement and flat ontology, we show how apps are part of socio-material configurations where they contribute to pregnancy meaning-making. We show how users experience a gradual realization that apps model ideal pregnancies and not the participants’ own, how the app's commercial nature conflicts with many participants' ideals of pregnancy, and how participants continue to use apps despite this but engage in selective use. All to support ways of making pregnancy in desired ways. Finally, we outline how users engage with pregnancy apps and the resulting natureculture arrangements where bodies and technology are intertwined and co-constructed. We argue that entanglement should be seen as a process, not a state, as users continually negotiate their use, engaging in what could be called processes of untangling.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2924–2933},
numpages = {10},
keywords = {HCI, Pregnancy, entanglement theory, reproductive health, user studies},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3664647.3681664,
author = {Xia, Kang and Li, Wenzhong and Shao, Yimiao and Lu, Sanglu},
title = {Vi2ACT:Video-enhanced Cross-modal Co-learning with Representation Conditional Discriminator for Few-shot Human Activity Recognition},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681664},
doi = {10.1145/3664647.3681664},
abstract = {Human Activity Recognition (HAR) as an emerging research field has attracted widespread academic attention due to its wide range of practical applications in areas such as healthcare, environmental monitoring, and sports training. Given the high cost of annotating sensor data, many unsupervised and semi-supervised methods have been applied to HAR to alleviate the problem of limited data. In this paper, we propose a novel video-enhanced cross-modal collaborative learning method, Vi2ACT, to address the issue of few-shot HAR. We introduce a new data augmentation approach that utilizes a text-to-video generation model to generate class-related videos. Subsequently, a large quantity of video semantic representations are obtained through fine-tuning the video encoder for cross-modal co-learning. Furthermore, to effectively align video semantic representations and time series representations, we enhance HAR at the representation-level using conditional Generative Adversarial Nets (cGAN). We design a novel Representation Conditional Discriminator that is trained to assess samples as originating from video representations rather than those generated by the time series encoder as accurately as possible. We conduct extensive experiments on four commonly used HAR datasets. The experimental results demonstrate that our method outperforms other baseline models in all few-shot scenarios.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {1848–1856},
numpages = {9},
keywords = {conditional generative adversarial nets, few-shot learning, human activity recognition},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3644479.3644482,
author = {Liu, Wan and Luo, Zhechong and Chen, Wan},
title = {Narrative Roads to Rebuild China's Global Image -Sentiment Analysis of Twitter Activities after China's Covid-19 Aid Activities in Italy},
year = {2024},
isbn = {9798400709333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644479.3644482},
doi = {10.1145/3644479.3644482},
abstract = {As the digital era unfolds, social media supported by mobile devices and AI technologies are rapidly changing the way people communicate. The impact of public opinion formed on the social network platforms often has an enormous and somehow long-lasting influence on a country's foreign policy and international relations, and international interactions in the form of Twiplomacy are gradually attracting the attention of researchers.However, the attempts to analyze the impact that online public opinion has on actual policy or the inverse are still lacking or failed to explain the problem on the socioeconomic ground. There are very few theoretical explanations of how different pandemic narratives emerges and co-exists, and yet no article that analyzed how the positive and negative narratives about China spread and wrestled in their respective public opinion arenas during such a global event.Under certain analogous sense, Virus like the rumor “China virus “and vaccines like the encouraging reports we get from the “China Aid Activities in Italy” are fighting each other in the social media battleground and thus in every individual's mind which finally aggregated as the ideology. Hence, to explain the underlying motives for the public to accept and spread different pandemic narratives, we combined the modified SIR model and Glaeser(2005) ' political economic dynamic to cast light on its deep route in the background of the U.S.-China trade conflict and the U.S. presidential election - all three Black swans landed in the same Lake.Our research combined the sentiment semantic analysis methods from the AI-NLP field with the causal mechanism analysis to evaluate the differences in the attitudes of the Italian &amp; international public towards China before and after the Chinese medical aid to Italy to fight the Covid-19 epidemic. Using the public data set from the GitHub project and other open-source scientific data, we applied the Bidirectional Encoder Representations from Transformers model (BERT) on individual tweets to include both emotional and non-emotional terms from a sentence sematic scale rather than word meaning scale which allows us to dig-in rich in topical information and estimate the international image trend of China during the 2020 February-March period. Here inspired by hybrid enrichment framework proposed by Gencoglu &amp; Gruber (2020) but limited with the data, we choose the ARMA model instead to infer the sentimental level and the causality of medical aid event, empirical result shows that the positive change in Italy' public sentiment level is significantly affected by China's aid activities event here. The results of this article also provide insight into how China can improve its international image to promote the progress of many international regional economic and trade cooperation from the narrative perspective. Facts always speak louder than words, with which the adequate speed-up dissemination of fact is also important.},
booktitle = {Proceedings of the 2023 6th International Conference on E-Business, Information Management and Computer Science},
pages = {12–24},
numpages = {13},
keywords = {COVID-19, Italy, Medical aid, Narratives, Sentiment analysis, Twitter},
location = {Hong Kong, Hong Kong},
series = {EBIMCS '23}
}

@inproceedings{10.1145/3677052.3698690,
author = {Fons, Elizabeth and Kaur, Rachneet and Zeng, Zhen and Palande, Soham and Balch, Tucker and Vyetrenko, Svitlana and Veloso, Manuela},
title = {TADACap: Time-series Adaptive Domain-Aware Captioning},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698690},
doi = {10.1145/3677052.3698690},
abstract = {While image captioning has gained significant attention, the potential of captioning time-series images, prevalent in areas like finance and healthcare, remains largely untapped. Existing time-series captioning methods typically offer generic, domain-agnostic descriptions of time-series shapes and struggle to adapt to new domains without substantial retraining. To address these limitations, we introduce TADACap, a retrieval-based framework to generate domain-aware captions for time-series images, capable of adapting to new domains without retraining. Building on TADACap, we propose a novel retrieval strategy that retrieves diverse image-caption pairs from a target domain database, namely TADACap-diverse. We benchmarked TADACap-diverse against state-of-the-art methods and ablation variants. TADACap-diverse demonstrates comparable semantic accuracy while requiring significantly less annotation effort.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {54–62},
numpages = {9},
keywords = {Adaptive, Domain-aware, Retrieval-based captioning, Time series captioning},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inbook{10.1145/3672608.3707751,
author = {Pasquadibisceglie, Vincenzo and Recchia, Vito and Appice, Annalisa and Malerba, Donato and Fiameni, Giuseppe},
title = {GANDALF: A LLM-based approach to map bark beetle outbreaks in semantic stories of Sentinel-2 images},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707751},
abstract = {Huge spruce forest areas have been damaged by massive bark beetle outbreaks across Europe during the past few years. Hence, forest health management requires large-scale inventory of bark beetle outbreaks to plan actions for promptly mitigating forest tree dieback. Deep learning techniques have recently achieved amazing results in imagery semantic segmentation tasks by dominating the recent research for mapping bark beetle outbreaks in Sentinel-2 images of forest areas. In addition, due to the impressive performance of Large Language Models (LLMs) in natural language understanding and generation tasks, LLMs have started attracting attention in multiple fields. In this paper, we describe GANDALF: an approach that leverages the potential of LLMs for mapping bark beetle outbreaks in Sentinel-2 images of forest areas. Specifically, we take advantage of the rich context of textual data to transform Sentinel-2 images in smart data ready for boosting accurate semantic segmentation modeling. We use a foundation LLM model to account for the text encoding of the spectral-spatial imagery context information. We fine-tune the LLM model to perform the semantic segmentation of forest images and use the Integrated Gradients (IG) algorithm to explain how each spectral-spatial information has an effect on the bark beetle outbreak detection. We assess the effectiveness of the proposed approach in a case study regarding bark beetle outbreaks in Sentinel-2 images of forest scenes in Czech Republic.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1074–1081},
numpages = {8}
}

@inproceedings{10.1145/3700824.3701097,
author = {Finol, Gerard and Gabriel, Arnau and Garc\'{\i}a-L\'{o}pez, Pedro and Gracia-Tinedo, Ra\'{u}l and Liu, Luis and Docea, Reuben and Kirchner, Max and Bodenstedt, Sebastian},
title = {StreamSense: Policy-driven Semantic Video Search in Streaming Systems},
year = {2024},
isbn = {9798400713194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700824.3701097},
doi = {10.1145/3700824.3701097},
abstract = {Streaming systems are an increasingly appealing substrate for managing video data via the stream abstraction. However, if we consider a large stream collection, it can be hard for data scientists to discover and locate relevant videos, let alone specific video fragments. In this paper, we propose StreamSense: a policy-driven, semantic video search solution for streaming systems. StreamSense allows users to deploy AI models that generate embeddings from video frames via policies. Our system uses such embeddings for building a two-level index in a vector DB that efficiently handles inter/intra video queries. StreamSense abstracts users from vector DB interactions so they can perform semantic search using images as input and visualize the results. We built our prototype on top of a tiered streaming storage system (Pravega) and validated it on a health-related use case. We show that StreamSense allows data scientists to search for video fragments in real surgery datasets in &lt; 30ms. StreamSense also reduces data ingestion related to AI training data loading in +80% compared to simple bulk loading video streams.},
booktitle = {Proceedings of the 25th International Middleware Conference Industrial Track},
pages = {29–35},
numpages = {7},
keywords = {data streams, semantic search, vector embeddings, video analytics},
location = {Hong Kong, Hong Kong},
series = {Middleware Industrial Track '24}
}

@article{10.1145/3653070,
author = {Mai, Gengchen and Huang, Weiming and Sun, Jin and Song, Suhang and Mishra, Deepak and Liu, Ninghao and Gao, Song and Liu, Tianming and Cong, Gao and Hu, Yingjie and Cundy, Chris and Li, Ziyuan and Zhu, Rui and Lao, Ni},
title = {On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper)},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2374-0353},
url = {https://doi.org/10.1145/3653070},
doi = {10.1145/3653070},
abstract = {Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have not yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial domains, including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality, such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, the task-agnostic large learning models (LLMs) can outperform task-specific fully supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image–based urban noise intensity classification, and remote sensing image scene classification), existing FMs still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing an FM for GeoAI is to address the multimodal nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal FM that can reason over various types of geospatial data through geospatial alignments. We conclude this article by discussing the unique risks and challenges to developing such a model for GeoAI.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jul,
articleno = {11},
numpages = {46},
keywords = {Foundation models, geospatial artificial intelligence, multimodal learning}
}

@article{10.1145/3701613.3701617,
author = {Goldkuhl, G\"{o}ran},
title = {Aspects of E-Government Interoperation: Case Study Research for Discovery Beyond Interoperability as Layered Capability},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3701613.3701617},
doi = {10.1145/3701613.3701617},
abstract = {How different public agencies and their digital artifacts work together (often labeled as interoperability) is considered to be an important area for a successful evolution of e-government. Key knowledge in this area is the structuring into four different layers of interoperability: legal, organizational, semantic, and technical interoperability. This paper challenges this received view of digital interoperation. It does so through an in-depth qualitative case study on the management of digital medical certificates and an inductive and abductive data analysis. Twelve aspects of digital interoperation have emerged through this study. The aspects have been divided into two categories: intrinsic (aspects that directly relate to the digital transfer of information) and extrinsic (aspects that shape different parts of digital interoperation from the outside). Six intrinsic aspects have been identified (relational, performative, semantic, informational, architectural, and technical). These intrinsic categories also have roles as extrinsic. Besides these, there are six more extrinsic aspects (normative, regulative, economic, cognitive, interactive, and temporal). Based on empirical insights and theorizing, the paper argues for a broadening of interoperability as not only the capability to interoperate. A broader notion comprises a mix of organizational, human, and digital pre-conditions for interoperation.},
journal = {SIGMIS Database},
month = oct,
pages = {53–79},
numpages = {27},
keywords = {cross-organizational business., enterprise interoperability, it governance}
}

@inproceedings{10.1145/3696410.3714816,
author = {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen, Huajun and Zhang, Wen},
title = {OntoTune: Ontology-Driven Self-training for Aligning Large Language Models},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714816},
doi = {10.1145/3696410.3714816},
abstract = {Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at https://github.com/zjukg/OntoTune.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {119–133},
numpages = {15},
keywords = {align with ontology, large language model, self-training},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3664647.3680806,
author = {Lin, Xun and Yu, Yi and Yu, Zitong and Meng, Ruohan and Zhou, Jiale and Liu, Ajian and Liu, Yizhong and Wang, Shuai and Tang, Wenzhong and Lei, Zhen and Kot, Alex},
title = {HideMIA: Hidden Wavelet Mining for Privacy-Enhancing Medical Image Analysis},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680806},
doi = {10.1145/3664647.3680806},
abstract = {Despite the advancements that deep learning has brought to medical image analysis (MIA), protecting the privacy of images remains a challenge. In a client-server MIA framework, especially after deployment, patients' private medical images can be easily captured by attackers from the transmission channel or malicious third-party servers. Previous MIA privacy-enhancing methods, whether based on distortion or homomorphic encryption, expose the fact that the transmitted images are medical images or transform the images into semantic-lacking noise. This tends to alert attackers, thereby falling into a cat-and-mouse game of theft and protection. To address this issue, we propose a covert MIA framework based on deep image hiding, namely HideMIA, which secures medical images by embedding them within natural cover images that are unlikely to raise suspicion. By directly analyzing the hidden medical images in the steganographic domain, HideMIA makes it difficult for attackers to notice the presence of medical images. Specifically, we propose the Mixture-of-Difference-Convolutions (MoDC) and Asymmetric Wavelet Attention (AsyWA) to enable HideMIA to conduct fine-grained analysis on each wavelet sub-band within the steganographic domain, mining features that are specific to medical images. Moreover, to reduce resource consumption on client devices, we design function-aligned knowledge distillation to obtain a lightweight hiding network, namely LightIH. Extensive experiments on six medical datasets demonstrate that our HideMIA achieves superior MIA performance and protective imperceptibility on medical image segmentation and classification.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {8110–8119},
numpages = {10},
keywords = {image hiding, medical image analysis, privacy-enhancing},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3703356,
author = {Han, Dengke and Yan, Mingyu and Ye, Xiaochun and Fan, Dongrui},
title = {Characterizing and Understanding HGNN Training on GPUs},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3703356},
doi = {10.1145/3703356},
abstract = {Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process. To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct a comprehensive quantification and in-depth analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we reveal the performance bottlenecks and their underlying causes in different HGNN training scenarios and propose optimization guidelines from both software and hardware perspectives.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {9},
numpages = {25},
keywords = {Heterogeneous graph neural networks, graph neural networks training, characterization, quantitative analysis, optimization guidelines}
}

@inproceedings{10.1145/3686397.3686401,
author = {Abrar, Asir and Ashar, Aetesam Ali khan and Liu, Jiangjiang},
title = {A Survey on Early-Stage Dementia Detection Using Natural Language Processing: Datasets and Approaches},
year = {2024},
isbn = {9798400717345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686397.3686401},
doi = {10.1145/3686397.3686401},
abstract = {According to the World Health Organization, Dementia, a chronic, degenerative condition that affects 55 million people worldwide, is prevalent in seniors. To date, there is no cure for dementia, so recent advancements in this area emphasize the urgent need for detecting early symptoms to facilitate timely intervention strategies. As dementia starts by damaging neurons in parts of the brain responsible for memory, language, and thinking, the analysis of language sample could potentially offer a promising avenue for detecting subtle cognitive shifts that could allow the possible interventions to slow down the disease's progression and improve the life of the individuals. We document this paper to explore the potential of leveraging speech and text data for detecting early-stage dementia by leveraging semantic, syntactic, and acoustic features. This paper surveys natural language processing (NLP) techniques applied to speech, text, audio, and handwritten data in monolingual and multilingual settings, focusing on their potential to aid in the early detection and understanding of dementia. According to our study, most datasets available in this domain are in English, with the support vector machine being the most frequently used classification method. Interest is also growing in using large language models to identify the signs of cognitive decline based on language patterns.},
booktitle = {Proceedings of the 2024 8th International Conference on Information System and Data Mining},
pages = {21–27},
numpages = {7},
keywords = {Alzheimer's Disease, Cognitive Decline, Dementia, Dementia Datasets, GPT, Health, NLP, Text},
location = {
},
series = {ICISDM '24}
}

@article{10.1145/3731247,
author = {Wei, Lisi and Zhao, Libo and Zhang, Xiaoli},
title = {MAINet: Modality-Aware Interaction Network for Medical Image Fusion},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3731247},
doi = {10.1145/3731247},
abstract = {Due to the limitations of imaging sensors, obtaining a medical image that simultaneously captures both functional metabolic data and structural tissue details remains a significant challenge in clinical diagnosis. To address this, Multimodal Medical Image Fusion (MMIF) has emerged as an effective technique for integrating complementary information from multimodal source images, such as CT, PET, and SPECT, which is critical for providing a comprehensive understanding of both anatomical and functional aspects of the human body. One of the key challenges in MMIF is how to exchange and aggregate this multimodal information. This article rethinks MMIF by addressing the harmony of modality gaps and proposes a novel Modality-Aware Interaction Network (MAINet), which leverages cross-modal feature interaction and progressively fuses multiple features in graph space. Specifically, we introduce two key modules: the Cascade Modality Interaction (CMI) module and the Dual-Graph Learning (DGL) module. The CMI module, integrated within a multi-scale encoder with triple branches, facilitates complementary multimodal feature learning and provides beneficial feedback to enhance discriminative feature learning across modalities. In the decoding process, the DGL module aggregates hierarchical features in two distinct graph spaces, enabling global feature interactions. Moreover, the DGL module incorporates a bottom-up guidance mechanism, where deeper semantic features guide the learning of shallower detail features, thus improving the fusion process by enhancing both scale diversity and modality awareness for visual fidelity results. Experimental results on medical image datasets demonstrate the superiority of the proposed method over existing fusion approaches in both subjective and objective evaluations. We also validated the performance of the proposed method in applications such as infrared-visible image fusion and medical image segmentation.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {166},
numpages = {23},
keywords = {Multimodal fusion, Medical image fusion, Cascade modality interaction, Modality-awareness, Graph convolutional network}
}

@inproceedings{10.1145/3674399.3674459,
author = {Zhang, Congyue and Fu, Wenjie and Tian, Canrong and Cheng, Xu and Tian, Yuan and Yu, Hao},
title = {3D Convolutional Network based micro-gesture recognition},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674399.3674459},
doi = {10.1145/3674399.3674459},
abstract = {Micro action recognition is an important research area in human motion analysis, which can be applied to the fields of healthcare, motion analysis and human-computer interaction. In this paper, we propose a 3D convolutional neural network-based micro-gesture recognition network to improve the performance, which combines skeletal and semantic embedding losses to enhance network discrimination. Specifically, first, various data enhancement techniques, such as level flipping and random Gaussian noise, are utilized to improve the robustness and generalization of the model. Horizontal flipping aims to help the model better recognise mirrored and stacked features in images by flipping them on the symmetry axis. Meanwhile, Gaussian noise, also known as white noise, is a set of random values with a normal distribution designed to force the model to learn features that are robust to small variations in the input, which can represent smudges or subtle absences in the image. We have evaluated our methods on a micro-posture recognition benchmark dataset, and these methods improve on previous methods. The model was tested on the iMiGUE dataset and achieved a Top1% accuracy of 59.01%—a notable improvement over other models.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024},
pages = {193–198},
numpages = {6},
keywords = {Micro-posture recognition, action classification., skeletal loss, skeleton-based action recognition, video understanding},
location = {Changsha, China},
series = {ACM-TURC '24}
}

@article{10.1145/3653018,
author = {Liang, Han and Chen, Jincai and Khan, Fazlullah and Srivastava, Gautam and Zeng, Jiangfeng},
title = {Audio-Visual Event Localization using Multi-task Hybrid Attention Networks for Smart Healthcare Systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1533-5399},
url = {https://doi.org/10.1145/3653018},
doi = {10.1145/3653018},
abstract = {Human perception heavily relies on two primary senses: vision and hearing, which are closely inter-connected and capable of complementing each other. Consequently, various multimodal learning tasks have emerged, with audio-visual event localization (AVEL) being a prominent example. AVEL is a popular task within the realm of multimodal learning, with the primary objective of identifying the presence of events within each video segment and predicting their respective categories. This task holds significant utility in domains such as healthcare monitoring and surveillance, among others. Generally speaking, audio-visual co-learning offers a more comprehensive information landscape compared to single-modal learning, as it allows for a more holistic perception of ambient information, aligning with real-world applications. Nevertheless, the inherent heterogeneity of audio and visual data can introduce challenges related to event semantics inconsistency, potentially leading to incorrect predictions. To track these challenges, we propose a multi-task hybrid attention network (MHAN) to acquire high-quality representation for multimodal data. Specifically, our network incorporates hybrid attention of uni- and parallel cross-modal (HAUC) modules, which consists of a uni-modal attention block and a parallel cross-modal attention block, leveraging multimodal complementary and hidden information for better representation. Furthermore, we advocate for the use of a uni-modal visual task as auxiliary supervision to enhance the performance of multimodal tasks employing a multi-task learning strategy. Our proposed model has been proven to outperform the state-of-the-art results based on extensive experiments conducted on the AVE dataset.},
note = {Just Accepted},
journal = {ACM Trans. Internet Technol.},
month = mar,
keywords = {hybrid attention, parallel attention, multi-task learning, healthcare monitoring}
}

@inproceedings{10.1145/3677892.3677950,
author = {Li, Xueshun and Zhang, Ruinan and Wang, Taiyang and Dong, Yu and Chen, Yang},
title = {Using Machine Learning to Optimize the Visual Perceptual Environment of Inter-house Leisure Spaces in Cold Winter Environments},
year = {2024},
isbn = {9798400709838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677892.3677950},
doi = {10.1145/3677892.3677950},
abstract = {In the winter environment of cold regions, the visual perception of inter-house leisure space in settlements has an important impact on the physical and mental health of residents. The study used an eye-tracking device combined with a semantic differential (SD) questionnaire to screen the visual attention elements and attributes of winter residents in cold regions, including the spatial aspect ratio (D/H), residential elevation saturation (RES), and the percentage of lawn in the field of view (POL). Orthogonal experiments were established in an immersive virtual environment to reveal the influence mechanism of visual perceptual environmental factors of cold regions settlements on residents' leisure space evaluation in winter environment. The study trained and compared four visual perception machine learning agent models, combining genetic algorithms (GA) with k-nearest neighbor algorithms (KNN), resulting in optimized threshold ranges of D/H: 2.22-2.54, RES: 68.47-82.34, and POL: 10%-14% in winter environments.},
booktitle = {Proceedings of the 2024 International Conference on Digital Society and Artificial Intelligence},
pages = {370–378},
numpages = {9},
location = {Qingdao, China},
series = {DSAI '24}
}

@inproceedings{10.1145/3706598.3714058,
author = {Lim, Brian Y. and Cahaly, Joseph P. and Sng, Chester Y. F. and Chew, Adam},
title = {Diagrammatization and Abduction to Improve AI Interpretability With Domain-Aligned Explanations for Medical Diagnosis},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714058},
doi = {10.1145/3706598.3714058},
abstract = {Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. Investigating XAI for high-stakes medical diagnosis, we propose improving domain alignment with diagrammatic and abductive reasoning to reduce the interpretability gap. We developed DiagramNet to predict cardiac diagnoses from heart auscultation, select the best-fitting hypothesis based on criteria evaluation, and explain with clinically-relevant murmur diagrams. The ante-hoc interpretable model leverages domain-relevant ontology, representation, and reasoning process to increase trust in expert users. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better performance than baseline models. We demonstrate the interpretability and trustworthiness of diagrammatic, abductive explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred over technical saliency map explanations. This work contributes insights into providing domain-aligned explanations for user-centric XAI in complex domains.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {419},
numpages = {25},
keywords = {Explainable AI, diagrams, abductive explanations, medical diagnosis},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3703847.3703872,
author = {Zhao, Zhenping and Gao, Yong and Sun, Congfei and Wang, Jing and Zhou, Lanjun and Xu, Hongji},
title = {Application of large models in wearable device-based cardiovascular risk monitoring},
year = {2024},
isbn = {9798400709746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703847.3703872},
doi = {10.1145/3703847.3703872},
abstract = {Text matching method was used to identify the corresponding Framingham risk factors from each cardiovascular disease examination report, and the corresponding risk coefficient given by the Framingham heart risk score scale was used as the Framingham risk coefficient. Words similar to each Framingham risk factor were collected as similar risk factors, and the self-attention agency model was used to find corresponding similar risk factors from each cardiovascular disease examination report, and the corresponding similar risk coefficient was given. Text semantic analysis method was used to find out the risk factors from each cardiovascular disease examination report, and the corresponding risk coefficients were calculated. The three types of risk factors were identified and the basic information of patients were spliced and integrated to generate the initial risk analysis report, and the medical large language model was used to adjust the initial risk analysis report through the fine-tuning of prompt words, and the final risk analysis report was generated.},
booktitle = {Proceedings of the 2024 International Conference on Smart Healthcare and Wearable Intelligent Devices},
pages = {144–150},
numpages = {7},
keywords = {Medical model, Risk factors, cardiovascular disease},
location = {
},
series = {SHWID '24}
}

@inproceedings{10.1145/3707172.3707173,
author = {Neha, Fnu and Bansal, Arvind K.},
title = {An Integrated Deep Learning Model to Analyze CT Scans for Minimally Invasive Accurate Classification of T1a Small Renal Masses},
year = {2025},
isbn = {9798400717499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707172.3707173},
doi = {10.1145/3707172.3707173},
abstract = {Convolution and attention-based deep learning techniques are emerging as promising approaches for automated medical image classification and semantic segmentation for minimally invasive diagnosis of malignant masses in vital organs. However, current deep learning based automated classification techniques are suitable only for detecting renal masses greater than 4.0 cm in diameter, which have a higher probability of malignancy increasing the morbidity, costly disease-management, and the probability of early mortality. T1a Small renal masses (SRM) are less than 4.0 cm in diameter and have a much smaller probability of being malignant and are corrected with partial nephrectomy or ablation. Detection of SRMs poses significant challenges due to low contrast with the surrounding and irregular shapes. As the imaging techniques improve, automated medical image-analysis techniques need further improvement for diagnosing SRMs. In this study, we present an integrated deep learning model and an algorithm for diagnosing T1a SRMs. The model integrates convolution blocks with residual links for capturing local features without information loss, cross-channel attention to focus on important features, principal component analysis to prune low variance features for reduced error and improved computational efficiency, and a transformer-encoder comprising self-attention to maintain dependencies and global context across image-patches. We trained and evaluated our model on the KiTS19 and KiTS21 challenge datasets for three ranges {1.2 – 2.0, 2.0 – 3.0, 3.0 – 4.0}. The model achieved an accuracy of 98.1%, 98.5%, and 97.3% for the three ranges, respectively. The accuracy of classification results significantly outperforms other approaches.},
booktitle = {Proceedings of the 2024 9th International Conference on Biomedical Imaging, Signal Processing},
pages = {1–9},
numpages = {9},
keywords = {Automated diagnosis, CT scan, deep learning, image processing, kidney, medical image analysis, renal cancer, small renal masses},
location = {
},
series = {ICBSP '24}
}

